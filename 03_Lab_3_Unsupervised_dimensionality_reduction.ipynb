{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining and Exploration [INFR11007]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Unsupervised dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we look at various unsupervised dimensionality reduction methods. By reducing the dimensionality to two or three dimensions only, we can visualise the data e.g. by scatter plots.\n",
    "\n",
    "When we deal with labeled data, we hope that in the low-dimensional space the classes are well-separated, that is, the transformed low-dimensional data form clusters which corresond to the different classes.\n",
    "\n",
    "Similar to Lab 1, we make use of the [landsat satellite](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Landsat+Satellite%29) dataset which is 36-dimensional and comprises 6 classes.\n",
    "\n",
    "As always, we first need to import the required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import required packages \n",
    "from __future__ import division, print_function # Imports from __future__ since we're running Python 2\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import manifold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1 ==========\n",
    "Load the `landsat_train.csv` dataset into a `pandas` DataFrame called  `landsat_train` and display the shape of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2 ==========\n",
    "Load the dataset class names into a DataFrame called `landsat_labels`.  By using the `to_dict()` method convert this DataFrame into a single dictionary. Alternatively, you can make use of  the `DictReader()` function which should be loaded from the `csv` module. There is also the option to use `numpy's` `genfromtxt()` function. You are free to choose the method that suits you best, but you should inspect the final dictionary to make sure it looks as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to replace the label numbers in the `landsat_train` DataFrame with the corresponding class names. We can achieve that by using the `pandas` function `replace()`. The `inplace` argument determines whether the method alters the object it is called upon and returns nothing, or returns a new object (when `inplace` is set to `False`).  \n",
    "\n",
    "Execute the cell below which performs this replacement. The second line is used to show a random sample of 5 entries of the DataFrame for us to inspect the outcome of this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace label numbers with their names\n",
    "landsat_train.replace({'label' : landsat_labels_dict}, inplace=True)\n",
    "landsat_train.sample(n=5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to separate the input features from the labels and store them into two `numpy` arrays. \n",
    "\n",
    "We will need two things:\n",
    "* the `pandas` `drop()`  can be used to discard a specified axis in a DataFrame. We will use it to drop the `label` column.\n",
    "* the `values` attribute of a DataFrame contains all valus in a `numpy array` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = landsat_train.drop('label', axis=1).values # Input features\n",
    "y = landsat_train['label'].values # Labels\n",
    "print('Dimensionality of X: {}\\nDimensionality of y: {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General note on scikit-learn methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All supervised estimators in scikit-learn implement a `fit(X, y)` method which can be used fit the model, and a `predict(X)` method that, given unlabeled observations `X`, returns the predicted labels `y`.\n",
    "\n",
    "Unsupervised methods and transformations still implement `fit(X)` but not `predict(X)`. Instead, they implement a `transform(X)` method which can be called once a model has been fit to transform the data in `X`. Most often, these two steps can be combined together by using the `fit_transform(X)` method when available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3 ==========\n",
    "\n",
    "Display the means and standard deviations of the first 4 columns of `X`. *Hint: you want to compute the means and standard deviations across the columns of your arrays. Make sure you make appropriate use of the `axis` parameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing: feature standardisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature standardisation** is a pre-processing technique used to transform data so that they have zero mean and unit standard deviation. For many algorithms, this is a very important step for training models (both regression and classification). Read about feature standardisation in the lecture notes and [here](http://scikit-learn.org/stable/modules/preprocessing.html) and make sure you understand what kind of transformation this method applies to the data.\n",
    "\n",
    "As you might expect, scikit-learn offers an [implementation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) of feature standardisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4 ========== \n",
    "\n",
    "Create a `StandardScaler` and fit it by using `X`. Then transform the array by using the scaler you just fit and save the results in a new array `X_sc`.\n",
    "\n",
    "Finally, display the means and standard deviations of the first 4 columns of `X_sc`. Do the look as expected?\n",
    "\n",
    "**For the rest of this lab you should use the standardised data (i.e. `X_sc`), unless you are explicitly asked to do otherwise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*\n",
    "\n",
    "Results look as expected, features have now zero mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "In this lab we will be using various dimensionality reduction methods and visualise their outputs in 2D space by using scatter plots. For this, we provide a function `scatter_2d_label()` which creates a scatter plot of 2D data and also annotates the corresponding classes appropriately. Execute the following cell and make sure you understand what this function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scatter_2d_label(X_2d, y, s=2, alpha=0.5, lw=2):\n",
    "    \"\"\"Visualuse a 2D embedding with corresponding labels.\n",
    "    \n",
    "    X_2d : ndarray, shape (n_samples,2)\n",
    "        Low-dimensional feature representation.\n",
    "    \n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Labels corresponding to the entries in X_2d.\n",
    "        \n",
    "    s : float\n",
    "        Marker size for scatter plot.\n",
    "    \n",
    "    alpha : float\n",
    "        Transparency for scatter plot.\n",
    "        \n",
    "    lw : float\n",
    "        Linewidth for scatter plot.\n",
    "    \"\"\"\n",
    "    targets = np.unique(y)\n",
    "    colors = sns.color_palette(n_colors=targets.size)\n",
    "    for color, target in zip(colors, targets):\n",
    "        plt.scatter(X_2d[y == target, 0], X_2d[y == target, 1], color=color, label=target, s=s, alpha=alpha, lw=lw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell selects two columns of `X_sc` (i.e. features in the high-dimensional space) and uses the `scatter_2d_label()` function provided above to visualise the 2D scatter plots. Feel free to experiment with different selections of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_1 = 19 # First dimension\n",
    "dim_2 = 25 # Second dimension\n",
    "plt.figure(figsize=(8,5)) # Initialise a figure instance with defined size\n",
    "scatter_2d_label(X[:, [dim_1,dim_2]], y)\n",
    "plt.legend(loc='center left', bbox_to_anchor=[1.01, 0.5], scatterpoints=3) # Add a legend outside the plot at specified point\n",
    "plt.xlabel('Dim {}'.format(dim_1))\n",
    "plt.ylabel('Dim {}'.format(dim_2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 5 ========== \n",
    "Take a random 2D projection of the data by using all the original features and observe the 2D scatter plot in the embedded space. Do you observe any clusters?  \n",
    "\n",
    "*Hint: set the `random_seed` parameter to an integer of your choice to ensure reproducible results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 2, we saw how to implement principal component analysis (PCA). In this lab, we will use the [`PCA`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) module in scikit-learn for dimensionality reduction. The cell below computes the PC scores from the (standardised) input data and makes use of the `scatter_2d_label()` function to visualise the outcome. Execute it and observe the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # Import the PCA module\n",
    "X_pca_2d = PCA(n_components=2).fit_transform(X_sc) # Initialise a PCA instance, fit it by using X_sc and then transform X_sc\n",
    "plt.figure(figsize=(8,5))\n",
    "scatter_2d_label(X_pca_2d, y)\n",
    "plt.title('Labelled data in 2-D PCA space')\n",
    "plt.xlabel('Principal component score 1')\n",
    "plt.ylabel('Principal component score 2')\n",
    "plt.legend(loc='best', scatterpoints=3) # Ask matplotlib to place the legend where it thinks best\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimate (KDE) in embedded space\n",
    "Some times we might want to describe the data in the embedded space in a slightly better way than with just a scatter plot. For instance, we might be interested in where the density is.\n",
    "\n",
    "### ========== Question 6 ========== \n",
    "By using the `scatter_2d_label()` function as a template, write a new function `kde_2d_label()` which shows kernily density estimates for the data in the embedded space. Individual densities should be estimated for each class and they should be shown in different colours.\n",
    "\n",
    "*Hint. You should make use of the seaborn function `kdeplot()`.*\n",
    "\n",
    "Next, use the function you just wrote to visualise the kernel density estimates of the PC scores for each class. Do not worry about adding a legend in your plot at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel principal component analysis (KPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a linear method, in the sense that the reduced dimension representation is generated by linear projections and this can severely limit the usefulness of the approach. Several versions of nonlinear PCA have been proposed in the hope of overcoming this problem. One such algorithm is kernel PCA.\n",
    "\n",
    "Kernel PCA applies the kernel trick to create a nonlinear version of PCA in sample space by performing ordinary PCA in the augmented kernel space (cf Lab 2, eigendecomposition of Gram matrix, and the lecture notes on kernel PCA).\n",
    "\n",
    "Scikit-learn offers an implentation of KPCA which supports the use of various kernels.  Familiarise yourself with this class by reading the [user guide](http://scikit-learn.org/stable/modules/decomposition.html#kernel-pca) and  [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 7 ========== \n",
    "\n",
    "By using the `KernelPCA` module, apply kernel PCA to the standardised data in `X_sc`. Set the `n_components` parameter to `2` and use default settings for other parameters. Experiment with different kernels. How do the results differ when different kernels are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 8 [optional] ========== \n",
    "\n",
    "Apply PCA and kernel PCA by using an RBF kernel to:\n",
    "1. the raw data in `X`\n",
    "2. the standardised data in `X_sc`. \n",
    "\n",
    "What do you observe? Which case seems to be affected by feature standardisation? Can you explain why?\n",
    "\n",
    "*Hint: Set the `n_components` parameter to `2` and use default settings for other parameters. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional scaling (MDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional scaling (MDS) refers to a collection of techniques for dimensionality reduction that operate on dissimilarities. The goal of MDS is to find a configuration of points in the plane, or more generally the Euclidean space, so that their distances well represent the original dissimilarities.\n",
    "\n",
    "We look here into metric MDS (see Section 3.3.1 in the lecture notes). Scikit-learn offers an implentation.  Familiarise yourself with this class by reading the [user guide](http://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling) and  [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 9 ========== \n",
    "Use the `MDS` module in scikit-learn to transform the standardised data by using metric MDS. Set `n_components` to `2`, `max_iter` to `100`, `n_init` to `1`, and use default settings for other parameters.  \n",
    "\n",
    "Visualise the data by using the `scatter_2d_label()` function. What do you observe?\n",
    "\n",
    "The code returns the value of the objective in (3.31), which is called the `stress_` score and corresponds to the quality of the lower dimensional representation (small values are better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 10 [optional] ========== \n",
    "Observe the outcome of metric MDS by using a [cosine](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) distance this time.\n",
    "\n",
    "Make sure you set the `dissimilarity` argument appropriately when you construct the `MDS` instance, and you pass the distance matrix when you call the `fit_transform()` method.\n",
    "\n",
    "Feel free to experiment with other types of distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isomap\n",
    "MDS does not attempt to explicitly model the underlying data manifold. Isomap, on the other hand, addresses the dimensionality reduction problem by doing so. Suppose your data lie on a curve, but the curve is not a straight line. The key assumption made by Isomap is that the quantity of interest, when comparing two points, is the distance along the curve between the two points. \n",
    "\n",
    "In other words, Isomap performs MDS in the geodesic space of the nonlinear data manifold. The geodesic distances represent the shortest paths along the curved surface of the manifold measured as if the surface was flat. This can be approximated by a sequence of short steps between neighbouring sample points. Isomap then applies MDS to the geodesic rather than straight line distances to find a low-dimensional mapping that preserves these pairwise distances.\n",
    "\n",
    "To summarise, Isomap uses three steps:\n",
    "1.  Find the neighbours of each data point in high-dimensional data\n",
    "2.  Compute the geodesic pairwise distances between all points\n",
    "3.  Embed the data via MDS so as to preserve these distances.\n",
    "\n",
    "\n",
    "Familiarise yourself with the Isomap class in scikit-learn by reading the [user guide](http://scikit-learn.org/stable/modules/manifold.html#isomap) and  [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 11  ========== \n",
    "\n",
    "Project the standardised data into a 2D space by using the Isomap algorithm. Set the `n_components` argument to 2 and explore the role of the `n_neighbours` parameter which defines how many neighbours are used in step 1 above. You can start by trying the following values, but feel free to experiment further: [1, 3, 5, 10, 50, 100]\n",
    "\n",
    "Use default settings for other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  t-distributed Stochastic Neighbor (t-SNE) Embedding\n",
    "\n",
    "t-SNE is a very powerful tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. \n",
    "\n",
    "t-SNE converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student’s t-distributions. The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space are minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence.\n",
    "\n",
    "One disadvantage of t-SNE is that it is computationally expensive.  Furthermore, the setting of the various hyper-parameters dramatically affects the results. Considering on top that the algorithm itself is not deterministic, it is obvious that setting these hyper-parameters can be a tricky task.\n",
    "\n",
    "The **perplexity** parameter is, in a sense, a guess about the number of close neighbors each point has. It is related to the number of nearest neighbors that is used in other manifold learning algorithms like Isomap. The original paper says *“The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50* but, as we will see, depending on the dataset, this parameter can dramatically affect the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 12  ========== \n",
    "\n",
    "Familiarise yourself with the t-SNE class in scikit-learn by reading the [user guide](http://scikit-learn.org/stable/modules/manifold.html#t-sne) and  [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
    "\n",
    "Apply t-SNE to the standardised data and explore the role of the `perplexity` input arugment. You can, but are not limited to, try the following values: [2, 5, 10, 30, 50, 100]. \n",
    "\n",
    "On top of each scatter plot, display the KL score achieved with each setting. *Hint: look into the `kl_divergence_` attribute of the model.*\n",
    "\n",
    "What do you think would be a good selection for the perplexity parameter?  \n",
    "\n",
    "How do you think the performance of t-SNE compares to that of the other methods? Is the distance between neighbouring clusters in the embedded space sensible?\n",
    "\n",
    "*You will probably notice that your code takes a while to execute (approx. 2-3 mins).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional\n",
    "\n",
    "The following webpage provides an excellent interactive introduction to t-SNE, feel free to check it out and experiment further with the various parameters of the algorithm. \n",
    "\n",
    "http://distill.pub/2016/misread-tsne/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dme_env]",
   "language": "python",
   "name": "conda-env-dme_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
